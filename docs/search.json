[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning diary CASA0023",
    "section": "",
    "text": "About the Author\nHello everyone. My name is Yudan Zhang, I am a Centre for Advanced Spatial Analysis graduate student studying MSc Urban Spatial Science from Chongqing, China.\nIn my undergraduate studies, I majored in Human Geography and Urban and Rural Planning, which I studied because I have a great love for geography, I like to discover geographic knowledge from life, and I also like to go to different places to investigate different landscapes, and humanities.\n\n\n\nI’m in the fjords of Tromsø, Norway.\n\n\nIn my undergraduate studies, I have been exposed to some remote sensing knowledge, studied basic remote sensing courses, and have done some research before, for example, I used VIIRS night lighting data and Impervious Surface Area (ISA) data to obtain the process of urban expansion in Chongqing from 2008-2018.\n\n\n\nMap of the urban land expansion of districts and counties in Chongqing during 2008–2018 (Zhang 2022)\n\n\nHowever, my experience in this field is still only using some mature remote sensing products, and my understanding of the process of remote sensing data acquisition, pre-processing, and analysing is very weak, and I hope to improve my knowledge of remote sensing in the course of this study. In addition, more importantly, I believe that remote sensing, as a research tool, can be of great help to environmental monitoring and ecological security, which is very much in line with the world’s theme of dealing with the climate crisis and low-carbon development, and I am very happy to use remote sensing as a key to open the door of my environmental research, and to make contributions to sustainable development."
  },
  {
    "objectID": "week2.html#xaringan-presentation---ikonos-2",
    "href": "week2.html#xaringan-presentation---ikonos-2",
    "title": "2  week2",
    "section": "2.1 Xaringan Presentation - IKONOS-2",
    "text": "2.1 Xaringan Presentation - IKONOS-2\nThe content for week 2 is a presentation in Xaringan on the Ikonos-2 satellite. The presentation is below, and the summary, application and reflection are all included in the slides."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "3  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "1  week1",
    "section": "",
    "text": "1. Summary"
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "1  Week1-Introduction",
    "section": "1.1 Summary",
    "text": "1.1 Summary\n\n1.1.1 Active vs Passive\nThe content of this chapter focuses on the working principles of active and passive remote sensing.\n\nTable 1. Comparison of the characteristics of active and passive remote sensing\n\n\n\n\n\n\n\n\nPassive\nActive\n\n\n\n\nEnergy use\nUse existing energy, do not emit anything\nHas an illuminated energy source\n\n\nDetection method\nDetects energy reflected from the sun, receives electromagnetic radiation emitted or reflected by the observed object itself\nActively emits electronically mediated waves and then waits for reception\n\n\nExamples\nHuman eyes, cameras, satellite sensors\nRadar, x-ray, lidar\n\n\n\nBased on respective characteristics, we are able to summarize the benefits and limitations of each of the two types of remote sensing.\n\nTable 2. Comparison of advantages and disadvantages of active remote sensing\n\n\n\n\n\n\nBenefits\nLimitations\n\n\n\n\n1. Unaffected by sunlight and able to observe both day and night.\n1. Reflected signals may be interfered with by other objects in nature or by electromagnetic noise.\n\n\n2. More sensors, need to adjust the wavelength of emission, able to measure height, terrain, wind, etc.\n2. Requires its own energy to transmit signals, limiting its use in drones or satellite platforms.\n\n\n3. Have penetration capabilities such as SAR to penetrate clouds and vegetation to obtain information on surface or subsurface structures.\n\n\n\n\n\nTable 3. Comparison of advantages and disadvantages of passive remote sensing\n\n\n\n\n\n\nBenefits\nLimitations\n\n\n\n\n1. Low energy consumption, requiring less energy, long mission times, low manufacturing and operating costs, and can be easily deployed and utilized on a large scale.\n1. Dependent on lighting conditions, performance is limited under poor ambient light conditions. The spectral range is also dependent on the spectral properties of the natural light source, resulting in limitations in observing certain specific wavelength bands.\n\n\n2. Capable of capturing a wide range of spectral information using sunlight, it is suitable for multiple scenarios.\n2. The received irradiance is mainly off-water irradiance, which can only be observed in specific atmospheric windows, and signals in other bands will be absorbed and masked. Susceptible to atmospheric components.\n\n\n\n\nFuture Development Trends\n\n\nMiniaturization and refinement of equipment: I believe that remote sensing equipment needs to address its own limitations, for example, active remote sensing equipment needs to become smaller and lighter in order to be deployed on a wide range of platforms. Similarly, passive remote sensing should be widely used in low-cost, high-frequency Earth observation missions.\nContinued integration with machine learning: Advanced algorithms are indispensable for both processing and analyzing remote sensing data. Deep learning models such as convolutional neural networks (CNN) are now widely used to automatically identify and classify features from active remote sensing data.\n\n\n\n\nFigure 1. Active and passive microwave satellites are widely used on global and regional scales to invert surface soil moisture changes (Mousa et al. 2020)\n\n\nIn the future, there is a need to develop more optimized machine learning models that combine active and passive remote sensing data to be able to process and interpret remote sensing data more accurately.\n\n\n1.1.2 Consider how electromagnetic waves interact\n\nInteractions at the Earth’s surface\n\n\nAbsorption by surfaces: Surface materials absorb electromagnetic waves of certain wavelengths. For example, vegetation strongly absorbs infrared light but reflects near-infrared light. By observing the absorption properties at specific wavelengths, the nature and condition of the surface material can be inferred.\nTransmission through the surface: When electromagnetic waves such as sunlight reach the Earth’s surface, they are reflected back into space. Different surface types, such as water bodies, vegetation, and urban buildings, have different reflectance properties that can be quantified by a bidirectional reflectance distribution function (BRDF). By analyzing the reflectance at different wavelengths, surface materials can be identified and classified.\n\n\nAtmospheric interactions\n\n\nScattering: As electromagnetic waves pass through the atmosphere, they scatter with gas molecules, water droplets, and other particles in the atmosphere. For example, Rayleigh scattering gives the sky a blue color. This scattering changes the direction and intensity of electromagnetic waves and affects the quality of remotely sensed data.\nCountermeasure: Removal of the sensor’s exposure to atmospheric scattering using synthetic aperture radar (SAR) and through atmospheric correction\n\n\n\n\nFigure 2. Helping to Understand the Principles of Observations Using the Electromagnetic Spectrum (NASA Science)\n\n\n\n\n1.1.3 Resolution\nWe explored 4 resolutions for RS data\n\nTable 4. Distinction between four remote sensing resolutions\n\n\n\n\n\n\nResolution\nCharacteristic\n\n\n\n\nSpectral\nObtaining values for each wavelength (or band of multiple wavelengths) in the electromagnetic spectrum to create spectral features, which may be discrete (multispectral) or continuous (hyperspectral)\n\n\nSpatial\nThe size of the raster image element. In most cases, RS data are rasters\n\n\nTemporal\nHow frequently surface data are updated, the time interval between revisits to the same location by a sensor\n\n\nRadiometric\nThe ability of a sensor to recognize and display small differences in energy (an 8-bit image can represent 256 different gray levels (or colors) and the transition from dark to light is smoother and more subtle changes can be captured).\n\n\n\nNotably, sensors that are able to resolve narrower band widths, have more bands, cover a wider spectral range, and have a finer band layout for key spectral features are considered to have higher spectral resolution.\nOften temporal and spatial resolution together influence the suitability of remotely sensed data for a particular application, e.g. a sensor with a low spatial resolution but a high temporal resolution (MODIS) is better suited for tracking and monitoring environmental change or agricultural development, etc.\n\n\n\nFigure 3. Different spatial resolutions are available for specific observation or research needs (NASA Earthdata)"
  },
  {
    "objectID": "week1.html#education",
    "href": "week1.html#education",
    "title": "1  week1",
    "section": "1.2 Education",
    "text": "1.2 Education\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "week1.html#experience",
    "href": "week1.html#experience",
    "title": "1  week1",
    "section": "1.3 Experience",
    "text": "1.3 Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "week1.html#applications",
    "href": "week1.html#applications",
    "title": "1  Week1-Introduction",
    "section": "1.2 Applications",
    "text": "1.2 Applications\n\n1.2.1 Remote sensing applications are characterised by inter-temporal and spatial features\nFirstly, remote sensing technology can be applied to Earth issues at different spatial scales, from a small pixel to a scene, from a region to a country, and has become the main way of global Earth observation.\n\n\n\nFigure 4. The use of the Sunglint phenomenon (sunlight reflected off the surface of the water to make the water appear grey, silver or white) can help to monitor the pattern of currents on the surface of the ocean around the Canary Islands (LANCE/EOSDIS MODIS Rapid Response Team).\n\n\nIn addition, remote sensing has the technology to analyse across time, and dynamic monitoring of indicators such as temperature, precipitation, biomass and other indicators in nature or targets such as lakes, rivers and roads can be accomplished in instantaneous or long time series, which can be determined by the temporal resolution of different remote sensing.\n\n\n1.2.2 Remote sensing technology has been used in all aspects of natural resources and human activities.\nIn the field of environment remote sensing can perform a series of environmental monitoring including soil erosion, water pollution, urban heat island, etc. Xu et al. (Xu & Chen 2004) used 6th band acquisition of thermal remote sensing data from Landsat TM/ETM+ imagery to observe the changes of urban heat island phenomenon in Xiamen, China within 11 years. In resource management remote sensing can be engaged in geological structure identification, glacier, coastline and island change monitoring, etc. Ke et al. (Ke et al. 2016) demonstrated different types of glaciers in three typical glacier-intensive subregions of the plateau based on elevation data from ICESat and Landsat images. In disaster prevention and mitigation, the role of remote sensing in monitoring extreme weather such as El Niño is irreplaceable.Hoque et al. (Hoque et al. 2017) suggest that object-based image analysis using optical images with a resolution of up to 30 metres can provide better impact assessment and recovery from tropical cyclones.\nI believe that the role of remote sensing in global environmental monitoring and response to climate change will be further enhanced as the impacts of global environment and climate change become more significant.\n\n\n\nFigure 5. Recognition applications of remote sensing also need to be continually refined. This Himalayan MODIS image of 1 November 2013 reflects the fact that it is not always possible to distinguish between haze, fog, clouds and snow in the visual interpretation of satellite imagery (How to Interpret a Satellite Image: Five Tips and Strategies)"
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "1  Week1-Introduction",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nI had been exposed to the course Introduction to Remote Sensing in my undergraduate studies, but had less practical use of remote sensing. Having used a combination of NPP-VIIRS nighttime lighting data and impervious surface data to define the extent of urban built-up areas in my undergraduate projects, I have developed a unique interest in the process of using remote sensing imagery to portray human activity. For this course, I think it can teach me well about how remote sensing technology can be used to solve practical environmental problems in the context of sustainable development. This course has taught me about downloading and using different sensors and has given us the opportunity to utilise platforms such as SNAP to manipulate Sentinel imagery, giving us very much hands-on opportunities.\nIn my future studies, I hope to get more exposure to different remote sensing sensors and methods of processing remote sensing data, applying them to practical research and integrating them with existing environmental policies, such as helping to meet carbon neutral targets, is something I hope to do and will make this course more meaningful."
  },
  {
    "objectID": "week1.html#references",
    "href": "week1.html#references",
    "title": "1  Week1-Introduction",
    "section": "1.4 References",
    "text": "1.4 References\nHoque, M.A.-A. et al., 2017. Tropical cyclone disaster management using remote sensing and spatial analysis: A review, International Journal of Disaster Risk Reduction. Amsterdam: Elsevier, årg. 22, s. 345–354.\nKe, L. et al., 2016. Remote sensing of glacier distribution and change over the Qinghai-Tibet Plateau [Online]. New York: IEEE.\nXu, H.Q. & Chen, B.Q., 2004. Remote sensing of the urban heat island and its changes in Xiamen City of SE China, International journal of disaster risk reduction, årg. 16, nr. 2, s. 276–281."
  },
  {
    "objectID": "week2.html#xaringan-presentation---npp-viirs",
    "href": "week2.html#xaringan-presentation---npp-viirs",
    "title": "2  Week2-NPP-VIIRS",
    "section": "2.1 Xaringan Presentation - NPP-VIIRS",
    "text": "2.1 Xaringan Presentation - NPP-VIIRS\nThe content for week 2 is a presentation in Xaringan on the Visible Infrared Imaging Radiometer Suite satellite (VIIRS). The presentation is below, and the summary (Summary of the sensor), application (Examples of research and its purpose) and reflection (Sensors use and how they can be used in future work)are all included in the slides."
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "3  Week3-Remote Sensing Data",
    "section": "3.1 Summary",
    "text": "3.1 Summary\n\n3.1.1 Image correction\nThe week began with a study of the different correction methods and their principles, which I will summarise in terms of their characteristics.\n\nGeometric correction and its Response Scenario: When collecting remote sensing data, there are image distortions due to viewing angle, terrain, wind, earth rotation, etc.\n\n\nTable 1. Main steps of geometric correction\n\n\n\n\n\n\n\nStep\nName\nDefinition\n\n\n\n\n1\nIdentify GCPs and transform coordinates\nIdentify ground control points (GCPs) in the image using data such as local maps. Derive geometric transformation coefficients in terms of the coordinates of the GCPs and use polynomial functions to map the distorted image coordinates to the correct geographic coordinates.\n\n\n2\nRegression analysis and optimisation of the fit\nTechniques such as linear regression are used to correlate distorted x or y coordinates with undistorted coordinates from the GCP. The root mean square error (RMSE) is calculated to measure the accuracy of the correction. Adjustments are made to minimise the RMSE.\n\n\n3\nResampling\nafter computing the transform coefficients, the image is resampled to match the corrected coordinates. Resampling methods include nearest neighbour, linear, cubic and cubic spline.\n\n\n\n\nTable 2. Geometric correction limitations\n\n\n\n\n\n\n\nNo.\nlimitations\nDefinition\n\n\n\n\n1\nDependence on GCPs\nIf these points are not of high quality (e.g., inaccurate positioning), this may lead to inaccurate results in the calibration process.\n\n\n2\nLoss of data quality due to resampling\nsome data quality may be lost due to interpolation during the resampling process. For example, nearest neighbour interpolation may lead to pixelation effects, and linear and cubic interpolation may introduce data smoothing.\n\n\n3\nAffected by environmental conditions\nIn complex or rapidly changing terrain (e.g., mountains or canyons), it may be difficult to obtain accurate geometric corrections, even when using GCPs. If the environment is rapidly changing, the final image may not accurately reflect current conditions, even with geometric correction.\n\n\n4\nHigher cost\nA large amount of manual input is required in selecting and validating GCPs. Being a computationally intensive process increases the overall resource and time requirements of the project.\n\n\n\n\nAtmospheric correction and its application scenarios: Used to eliminate the effect of atmospheric gases and particles on light captured by remote sensing instruments, as the atmosphere can scatter and absorb light and thus change the true reflectance value of the Earth’s surface as seen by the sensor.\n\n\nTable 3. Types of atmospheric corrections and characteristics\n\n\n\n\n\n\n\nType\nBenefit\nlimitation\n\n\n\n\nRelative correction\n1. This can be done using the image data itself without additional atmospheric information.\n1. May not be able to fully correct for all atmospheric effects when conditions vary greatly between images.\n2. Less suitable for quantitative analyses requiring precise reflectance values.\n\n\nAbsolute correction\n1. Provide more accurate corrections by using physical models of atmospheric properties.\n1. Detailed atmospheric data, such as local atmospheric visibility, are required\n2. Models must be carefully selected and calibrated to the specific conditions of the image.\n\n\nExperience line correction\n1. Provide more targeted calibration for the time and conditions of image acquisition.\n2. correctly collect in-situ measurements to ensure calibration accuracy.\n1. The conditions are demanding and the atmospheric conditions must match. The selected ground targets must be well distributed throughout the image and cover the range of reflectance values present in the scene.\n\n\n\n\n\n\nFigure 1. DigitalGlobe Atmospheric Compensation (DG AComp) original image (left) and atmospherically corrected image (right) (Lee & Yum 2019)\n\n\n\nOrthometric and Radiometric correction\n\n\nTable 4. Summary of the characteristics of orthographic and radiometric calibration\n\n\n\n\n\n\n\nCorrection\nBenefit\nlimitation\n\n\n\n\nOrthorectification\n1. The spatial accuracy of the imagery is improved to measure true distances, angles and areas.\n2. Can be seamlessly integrated into GIS, and deformed images become easier to interpret in tasks such as element extraction and land cover classification.\n1. If the DEM or sensor geometry is inaccurate, errors are introduced rather than corrected.\n\n\nRadiation correction\n1. In cases where the signal received by the sensor is affected by atmospheric conditions, angles, etc., radiometric correction reduces the impact on the quality of the data for comparison with different sensors at different times.\n1. Solar altitude angle affects the intensity of the radiation, and remotely sensed images are usually acquired at times when the solar altitude angle is relatively high and atmospheric conditions are stable. Properly performed radiometric corrections also require adequate sensor calibration information, such as gain and offset.\n\n\n\n\n\n3.1.2 Data joining and enhancement\n\nData joining\n\n\nThe counterpart to GIS merging in remote sensing is called tessellation. Feathering using a base image and other images is used to reduce the effect of seam lines.\nFeathering is different in different cases: usually based on representative samples and histogram matching algorithms it is possible to obtain similar luminance of two images for feathering\nComparison between multiple images on the same area or when processing images at different times\nUse normalisation and normalisation of Surface Reflectance (SR)\nUse Permanent Invariant Features (PIFs, Permanent Invariant Features) or other surface features that do not change over time to calibrate differences between multiple images.\n\n\nData enhancement\n\nIn order to improve the visual appearance or outcome, images can have different enhancements applied to them：\nSensors have a low band range in most of the images to avoid saturation, so methods like Minimum - Maximum, Percentage Linear and Standard Deviation, Piecewise Linear Contrast Stretch are used to add contrast.\nThere are also different ways to improve the image such as Ratio, Filtering, Principal Component Analysis, spatial variation of grey values, Data Fusion.\n\n\n\nFigure 2. Analysis of texture features in a set of bands of LANDSAT in New Delhi region using Gray-Level Co-Occurrence Matrix (GLCM) texture analysis\n\n\n\n\n\nFigure 3. Infrared image enhancement process including decomposition, correlation measurements, reflectance component enhancement, illumination component enhancement and image reconstruction (Chen et al. 2020)"
  },
  {
    "objectID": "week3.html#applications",
    "href": "week3.html#applications",
    "title": "3  Week3-Remote Sensing Data",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nSince remote sensing products are often already calibrated during the application phase, this time I focused on investigating the issue of image enhancement applications in the classroom.\n\nHow image enhancement methods can help the urban environment?\n\nThe computations such as image connectivity and enhancement learnt this week can be useful in real-world studies of urban environments. For example, in applications where edge enhancement of remotely sensed images is used to extract building footprints for monitoring natural disasters and urban development. The U-Net CNN architecture is one of the most basic networks in semantic segmentation, but such networks are not deep enough to detect detailed edge features.\nHoin Jung et al.(Jung, Choi & Kang 2022) used the edge features obtained by overall nested edge detection as inputs to their boundary enhancement module, where the two subunits of the module, detection boundary and segmentation mask, share information by combining probability graphs to represent the likelihood of the target object for boundary enhancement purposes. Compared with other semantic segmentation methods, the extracted architectural results of the proposed method in this study can have clear and subtle boundaries. In addition to extracting building information, the use of edge enhancement can also be used to obtain features of other features, such as the representation and extraction of spectral features of water bodies (Zhang & Lun 2023), which can help monitor the urban water environment.\n\nDoes increasing the complexity of the images (or creating new datasets) help us achieve our goals?\n\nContrast enhancement and filtering mainly improve or modify the visual quality or specific features of existing imagery, PCA and texture analysis may lead to the creation of new datasets and an increase in complexity.\nFor example, texture is defined by calculating a statistical measure of pixel values within a local area of the image, thus increasing the image dataset for texture measures. Dekker et al. (Dekker 2001)investigated the map updating capabilities of ERS SAR imagery in urban areas using texture measures and found mean intensity, variance, skewness, weighted rank fill rate, semi-variance function (or wavelet energy measure), and gap metrics several texture measures are capable of providing the best land cover separability for the Rotterdam area. Overall, enhancement techniques can increase the information content of the images in different ways, to some extent placing new demands on the analysis and interpretation of the data, but also accomplishing the appropriate goals for the research questions."
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "3  Week3-Remote Sensing Data",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nHonestly speaking, due to the previous exposure to corrected remote sensing products, this is the first time to learn the principles of remote sensing correction systematically.\nAs a matter of fact, correction procedures such as atmospheric correction and terrain correction are crucial for obtaining true information about the ground surface, just like in climate change research, failure to correctly correct for cloud cover or atmospheric effects may lead to misinterpretation of surface temperature and vegetation status. I believe that accurate image correction also ensures that decision makers can make more effective judgements in areas such as environmental monitoring, urban planning, and agricultural management.\nFor example, in the field of public safety, corrected images can be used in disaster response and emergency relief (Li et al. 2016) for real-time disaster monitoring. I believe that in the future, we can continue to extend image correction skills to different remote sensing platforms, such as UAVs or near-Earth observation, and thus improve the real-time and accuracy of disaster monitoring, which is very much in demand.\nAs mentioned in the application case, enhancement techniques such as PCA or filtering can be used to extract specific environmental features, which is very important in applied research in geological exploration, biodiversity. I also believe that integrating datasets of different formats, resolutions, and times is very necessary in practical research, because our research often requires the integration of a collection of remote sensing data from multiple sources as well as long time series in order to make the research more meaningful. Therefore, I found the data preprocessing and quality control methods learned in this literature to be very beneficial. In future research, I believe that integration with datasets from other disciplines (e.g. socio-economics, ecology) is necessary, which is the key to applying remote sensing to a wider range of real-world problems, and I have learnt that methods of augmenting datasets are constantly being updated, and I hope that I will be able to learn more advanced algorithms and models to deal with more complex datasets in the future, such as the use of machine learning techniques to automatically identify and classify surface features."
  },
  {
    "objectID": "week3.html#references",
    "href": "week3.html#references",
    "title": "3  Week3-Remote Sensing Data",
    "section": "3.4 References",
    "text": "3.4 References\nChen, J. et al., 2020. A Novel Infrared Image Enhancement Based on Correlation Measurement of Visible Image for Urban Traffic Surveillance Systems, Journal of Intelligent Transportation Systems. Taylor & Francis, årg. 24, nr. 3, s. 290–303.\n\nDekker, R.J., 2001. Texture analysis of urban areas in ERS SAR imagery for map updating [Online]. New York: IEEE.\n\nJung, H., Choi, H.-S. & Kang, M., 2022. Boundary Enhancement Semantic Segmentation for Building Extraction From Remote Sensed Image, IEEE Transactions on Geoscience and Remote Sensing, årg. 60, s. 1–12.\n\nLee, K.-H. & Yum, J.-M., 2019. A Review on Atmospheric Correction Technique Using Satellite Remote Sensing, Korean Journal of Remote Sensing. The Korean Society of Remote Sensing, årg. 35, nr. 6_1, s. 1011–1030.\n\nLi, Y. et al., 2016. Geometric correction algorithm of UAV remote sensing image for the emergency disaster [Online].\n\nZhang, Y. & Lun, H., 2023. Remote Sensing and Image Processing Techniques for Water Environment Monitoring: A Case Study of the Beijing-Tianjin-Hebei Region, TRAITEMENT DU SIGNAL. Edmonton: Int Information & Engineering Technology Assoc, årg. 40, nr. 4, s. 1771–1779."
  },
  {
    "objectID": "week4.html#summary-issues-and-policies",
    "href": "week4.html#summary-issues-and-policies",
    "title": "4  Week4-Policy Applications",
    "section": "4.1 Summary: Issues and policies",
    "text": "4.1 Summary: Issues and policies\n\n4.1.1 Global issues\nCarbon dioxide, one of the main causes of global climate change, is emitted mainly in urban areas. Urban areas account for more than 70 per cent of global CO2 emissions. According to Ribeiro et al. (Ribeiro, Rybski & Kropp 2019), the magnitude of carbon dioxide emissions is affected by population density in direct proportion to the size of the city.\n\n\n4.1.2 Study Area Context - Shanghai\nIn China, with rapid industrialisation, different cities are under pressure to address climate change. Shanghai is a typical cosmopolitan city, with population densities of more than 300 people/ha in Huangpu, Jing’an and Hongkou districts in 2018, while Putuo and Yangpu districts have population densities between 200-300 people/ha. Its status as the most industrialised and affluent city in China is also the most polluted (Li et al. 2017).\n\n\n\nFigure 1. Shanghai Ecological Network Plan (2017-2035) (Liu et al. 2021)\n\n\nAs a C40 city, Shanghai should actively follow the Paris Agreement’s 1.5˚C target to reduce greenhouse gas emissions and climate risks in the context of economic growth. Although Shanghai is missing from the C40 Cities Climate Leadership Group’s city climate change plan statistics (C40 Cities Climate Leadership Group & C40 Knowledge Hub 2022). However, the region also has local policies that actively address climate change, for example in the area of carbon emissions.\n\n\n4.1.3 Policy summary\nChina is currently leading the current global trend in carbon emissions along with other developing countries, setting a goal of achieving carbon neutrality by 2060 (The State Council of the People’s Republic of China 2021).\nShanghai, as the first Chinese city to enter the post-industrialisation phase, is striving to meet China’s energy intensity and pollution reduction targets, and regulations have been enacted at the city level, such as reducing energy consumption per unit of gross domestic product (GDP) by 14 per cent by 2025 compared to 2020, and striving to reach a 20 per cent share of non-fossil energy in total energy consumption. Conducting estimation and management of natural carbon stocks is one of its main tasks (Shanghai Municipal Government 2022).\n\nTable 1. Summary of carbon-neutral-related policies at different levels\n\n\n\n\n\n\n\nNo.\nPolicy\nAims\n\n\n\n\n1\nThe Paris Agreement\nThe Goal of limiting global warming to 1.5°C\n\n\n2\nC40 Leadership Standards\nContribute to halving overall emissions in C40 cities by 2030; mainstream their equitable climate goals into the decision-making processes of the most influential cities\n\n\n3\nChina Plan for Carbon Dioxide Peak\nPeak and steady decline in carbon dioxide emissions by 2030, and successful achievement of the carbon neutrality target by 2060\n\n\n4\nShanghai Carbon Peaking Action Plan\nBy 2030, non-fossil energy sources will strive to account for 25 per cent of total energy consumption, and carbon dioxide emissions per unit of gross domestic product (GDP) will be reduced by 70 per cent compared with 2005, ensuring that carbon peaks by 2030"
  },
  {
    "objectID": "week4.html#applications",
    "href": "week4.html#applications",
    "title": "4  week4",
    "section": "4.2 Applications",
    "text": "4.2 Applications"
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "4  week4",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection"
  },
  {
    "objectID": "week4.html#applications-addressing-policy-challenges-with-data",
    "href": "week4.html#applications-addressing-policy-challenges-with-data",
    "title": "4  Week4-Policy Applications",
    "section": "4.2 Applications: Addressing policy challenges with data",
    "text": "4.2 Applications: Addressing policy challenges with data\n\n4.2.1 Calculation of carbon emissions\nCarbon emissions in cities come from a variety of sources, and different levels of carbon emissions can be portrayed with different remote sensing tools. For example, the total nighttime light intensity value has a good correlation with carbon emissions. Zhang et al. (Zhang et al., 2024) calculated the carbon emissions in China by using the long-series nighttime light dataset based on the calibration and fusion of the DMSP/OLS annual monthly image data and the NPP/VIIRS monthly image data in conjunction with the total nighttime light value, simulation coefficients, and the simulation intercepts of the study area.\n\n\n\nFigure 2. Carbon emissions per capita at the provincial level in China (2000, 2005, 2010, 2015, 2019) (Zhang et al., 2024)\n\n\nBased on FROM-GLC10, the world’s first 10 m resolution global surface coverage product, Ji et al. (Ji et al., 2023) constructed partial least squares regression, random forest, and geo-detection models using factors affecting building emissions, and investigated the effects of building compactness on carbon emissions from building energy consumption at different buffer distances in 59 cities in China.\n\n\n4.2.2 Calculation of carbon sinks\nThe breakdown types in the field of carbon storage and emission are also classified into forest carbon, building carbon, transport carbon, industrial carbon, etc. based on the source. Zhang et al. (Zhang et al., 2023) downloaded Sentinel-2 images of forest cover during the growing season with cloud cover less than 4% as a benchmark, selected 4 spectral reflectance indices and 15 vegetation indices that are significantly related to forest carbon density as candidate indicators for carbon stock estimation, and used a combination of manual visual interpretation and field survey based on aerial photographs taken by the Survey and Mapping Institute to detect sinks in Chongming Island forest. The forests of Chongming Island were detected by a combination of manual visual interpretation and field survey based on aerial photographs of the Institute of Survey and Mapping.\n\n\n\nFigure 3. Spatial Distribution of Forest Carbon Density at Classification Scale for the Whole Forest of Chongming Eco-Island in 2020 (Zhang et al., 2023)\n\n\nAlthough the industrial structure of Shanghai is improving, it is still under pressure to reduce carbon emissions and energy consumption, so using remote sensing images to track its carbon emissions and other related activities is of great significance and more objective at the same time. Meanwhile, most of the studies are limited to energy emission or a single aspect, and there is a lack of research that systematically aggregates carbon emission related factors such as buildings, population, and industry, which is a direction that future studies can try. For future research, I would like to continue to explore the application of remote sensing to carbon sinks, which are becoming more and more important due to the decline of forests and grasslands in China."
  },
  {
    "objectID": "week4.html#reflection-related-to-policy-cities-and-data-applications",
    "href": "week4.html#reflection-related-to-policy-cities-and-data-applications",
    "title": "4  Week4-Policy Applications",
    "section": "4.3 Reflection: Related to policy, cities and data applications",
    "text": "4.3 Reflection: Related to policy, cities and data applications\nI think it’s very rewarding to bring together policy, cities and real geography, and just as remote sensing can contribute to the development of low carbon cities, I’m better able to understand the contribution our discipline can make to real life. This time we started at the policy level and needed to have a policy with cities. I found that although C40’s Cities for Climate Action statistics do not include the actions of cities such as Shanghai, Chinese cities such as Shanghai also have clear and ambitious climate action targets. It is precisely because of these targets such as Peak Carbon, Carbon Neutral, etc. that we need to use remote sensing data etc. to more objectively and accurately assess whether the actual level of carbon emissions is moving in the right direction.\nThis week I learned how to use the characteristics of remote sensing data such as Sentinel-2, NPP/VIIRS, etc. in conjunction with other data to portray various types of carbon emissions. Then to complete the estimation and management of carbon stocks at the city level, I believe that multiple remote sensing data are needed to improve the accuracy of the estimates, such as combining NPP data from MODIS with forest cover change data from Landsat to estimate carbon emissions or sequestration in a specific area. I believe that constructing accurate models to simulate and predict the carbon cycle process, combining remote sensing data for validation and optimisation is a continuing research hotspot in the future."
  },
  {
    "objectID": "week4.html#references",
    "href": "week4.html#references",
    "title": "4  Week4-Policy Applications",
    "section": "4.4 References",
    "text": "4.4 References\nC40 Cities Climate Leadership Group & C40 Knowledge Hub, 2022. C40 Cities with a Paris Climate Agreement compatible CAP.\nJi, R. et al., 2023. Green Space Compactness and Configuration to Reduce Carbon Emissions from Energy Use in Buildings, Remote Sensing. Multidisciplinary Digital Publishing Institute, årg. 15, nr. 6, s. 1502.\nLi, Z. et al., 2017. Towards low carbon based economic development: Shanghai as a C40 city, Science of The Total Environment, årg. 576, s. 538–548.\nRibeiro, H.V., Rybski, D. & Kropp, J.P., 2019. Effects of changing population or density on urban carbon dioxide emissions, Nature Communications. Nature Publishing Group, årg. 10, nr. 1, s. 3204.\nShanghai Municipal Government, 2022. Shanghai Carbon Peaking Action Plan. Shanghai, China.\nThe State Council of the People’s Republic of China, 2021. China Rolls Out Plan for Carbon Dioxide Peak.\nZhang, C. et al., 2023. Estimating the Forest Carbon Storage of Chongming Eco-Island, China, Using Multisource Remotely Sensed Data, Remote Sensing. Multidisciplinary Digital Publishing Institute, årg. 15, nr. 6, s. 1575.\nZhang, Z. et al., 2024. Spatiotemporal Analysis and Prediction of Carbon Emissions from Energy Consumption in China through Nighttime Light Remote Sensing, Remote Sensing. Multidisciplinary Digital Publishing Institute, årg. 16, nr. 1, s. 23."
  },
  {
    "objectID": "week5.html#summary",
    "href": "week5.html#summary",
    "title": "5  Week6-GEE",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThis week was a systematic introduction to the tool Google Earth Engine (GEE), symbolising the fact that we can now actually work with remotely sensed data and complete analyses, and I’m going to summarise what I’ve learned in class.\n\nTable 1. Summary of GEE characteristics\n\n\n\n\n\n\nFeature\nSpecific functions\n\n\n\n\nMade for geospatial analysis\n1. Allows large-scale geospatial analyses, storing data on the server. Because of the server side of the Earth Engine data and computing resources operations performed on Google servers.\n2. images are rasters, with band. features are vectors, with geometry and attributes\n\n\nClient-side and server-side\n3. Ability to apply written code (Javascript). In GEE, we have code that runs on the client side and includes tasks such as user interface operations, initiating data requests and processing user input. Ability to create searchable online applications\n4. “Earth Engine Objects” in GEE are the core components that represent various types of data and operations in Google Earth Engine. Prefixed with “ee,”, including everything from images and collections of images.\n5. The client-side code in GEE is what you write in your local environment, while the server-side code runs on GEE’s servers. The data doesn’t actually exist on the client side, and a loop such as a for loop that tries to loop through an object won’t know what it’s supposed to loop through. So GEE uses a mapping function that maps it to the server, where it is applied to each element in the collection.\n\n\nAutomation of coordinate systems and projections\n6. In GEE, we can control the scale (pixel resolution) at which analyses are performed, which may differ from the original resolution of the data (determined by the scale specified in the analysis or output). The system is managed through the use of image pyramids (image datasets in GEE are stored as pyramids of multiple resolutions) and resampling methods (nearest neighbours), which allow the flexibility to use different scales to meet a variety of analysis needs.\n7. Mercator projection is used by default and the user usually does not need to be concerned with projections in GEE. The platform is responsible for transforming and displaying all data in a common projection.\n\n\n\n\n5.1.1 Simple operations at GEE\n\nLearning to use objects\n\n\nIn GEE, vectors, rasters, elements, strings, numbers, etc. are all objects. Each object belongs to a class that defines attributes and operations that can be performed.\nThe simplest form of spatial data is the Geometry object, a geometric figure without attributes, which becomes a Feature after associating attributes.\nCollections: Feature collections are compilations of Features that can be used to represent more complex data sets with spatial and attribute data. Image collections are stacks of raster images. Allows the user to load, manipulate and analyse multiple images as a cohesive data set.\n\n\n\n\nFigure 1. Loading and displaying Landsat 9 satellite imagery using specific geographic locations (New Delhi), time frames (January and February 2020) and cloud coverage conditions (less than 10%)\n\n\n\nReduce image data\n\n\nReduction of image collections, using a function to extract the extreme or aggregated values for each pixel in the collection.\nRegion reduction, which generates statistical data for images within a specified study area.\nNeighbourhood reduction, a method of relating each pixel to the surrounding area, analyses the window of pixels around the central pixel.\n\n\n\n\nFigure 2. Cropping the region defined by Delhi, the three bands [‘SR_B4’, ‘SR_B3’, ‘SR_B2’] were selected for visualisation, corresponding to the red, green and blue bands, which produces images close to true colour.\n\n\n\nRegression\n\n\nPixel-level linear regression: one band as the dependent variable and another band as the independent variable (usually time)\nMultiple linear regression: contains multiple dependent and independent variables, similar to ordinary least squares (OLS) regression for each dependent variable.\nRegression using Reducers, performed pixel by pixel on a collection of images spanning several years, can also involve aggregating values within polygons using data from a single date. A constant must be added as an independent variable to account for the intercept.\n\n\nCombining different datasets\n\n\nSet: join two image collections based on common attributes or indexes; also can join different element collections together.\nJoins: implemented through in the filter. LeftField refers to the index or attribute in the primary dataset; RightField refers to the corresponding attribute in the secondary dataset.\n\n\nTable 2. Summary of joining types\n\n\n\n\n\n\nJoining type\nPrinciple\n\n\n\n\nSimple join\nMatches any record in the secondary set to the primary set.\n\n\nInverted join\nKeep any records in the primary set that have no matches in the secondary set.\n\n\nInternal join\nReturn all matching records between the two collections as a new element collection.\n\n\nSpatial join\nThe join attributes are stored in the attributes of the elements. Intersection operations can be performed and spatial subsetting of data is also possible.\n\n\n\n\n\n\nFigure 3. With regard to the analysis of power stations in the vicinity of protected areas, this includes the use of buffer zones (Extending the buffer zone by 100 KM outside the boundaries of the protected area, adding the buffer zone to the map in red) and spatial connectivity (Applying a spatial filter, filtering neighbouring elements by 100 KM distance, and saving the power stations that meet the criteria of the spatial filter to the attributes of the protected area feature)."
  },
  {
    "objectID": "week5.html#applications",
    "href": "week5.html#applications",
    "title": "5  Week6-GEE",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nSince this week is an introduction to the GEE platform, I’m going to explore why and how to use GEE for research:\n\n5.2.1 Reasons for using GEE\nIn traditional geographic information science (GIS) there are three main challenges(Yu & Gong 2012):\n\nTable 3. Three GIS challenges that GEE can address\n\n\n\n\n\n\nNo.\nChallenge\n\n\n\n\n1\nThree-dimensional global representation and visualisation of geospatial data\n\n\n2\nPre-processing and mining of spatial big data\n\n\n3\nSpatio-temporal techniques for studying geographic processes\n\n\n\nThe combination of Google Earth (2005) and GEE (2010) provides a platform for earth system science research and is an effective tool for the challenges faced by traditional GIS and meets the needs of global environmental change research. Compared to the visualisation capabilities of Google Earth, the cloud computing capability of GEE is an effective tool for the analysis of global geospatial big data (Amani et al. 2020). The feature of providing free services to support more geospatial data is especially helpful for geographic research in less developed regions.\n\n\n5.2.2 Research using GEE\nWith a long record of earth observations and algorithms for analysing geo-big data, GEE has a clear role in environmental monitoring and analysis. This includes all aspects of agriculture, water resources, disasters, climate change, forests, etc (Bullock, Woodcock & Olofsson 2020, Busker et al. 2019, Meilianda et al. 2019, Workie & Debella 2018, Xiong et al. 2017). As combating climate change and reducing carbon emissions is a long-term global issue, I think the GEE platform can continue to play a role in this field, and focusing on this aspect of research can also help me find more inspiration.\n\n\n\nFigure 4. Spatial distribution of Landsat optical and Sentinel-1A SAR data used to identify mangroves and associated water bodies(Chen et al. 2017)\n\n\nChen et al.(Chen et al. 2017)used time-series Landsat, which provides biophysical features such as identification of greenness, canopy cover, and tidal inundation to map mangrove forests in China, and used the integration of Sentinel-1A with the modified Normalized Difference Water Index (mNDWI) to identify mangrove-associated water bodies. The whole process included six regions, and 1941 Landsat images and 586 Sentinel-1A images were run on GEE with an accuracy greater than 95%. Mangrove forests, as one of the most carbon-rich forests in the tropics, are of great significance for the study of sustainable management and carbon stock. Hao et al. (Hao et al. 2019) estimated the interannual variations of surface temperature and seasonally integrated normalised vegetation index (NVI) of the Three Gorges catchment area, China, from 2000-2015 by using the Moderate Resolution Imaging Spectroradiometer (MODIS) product on the GEE. Long-term monitoring is easily possible using the GEE platform."
  },
  {
    "objectID": "week5.html#reflection",
    "href": "week5.html#reflection",
    "title": "5  Week6-GEE",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nThis week was my first exposure to the GEE platform and I learnt not only the features and some terminology of GEE allowing large-scale geospatial analyses, but also the process of data import, cropping, texture analysis, and PCA analyses, combining it with what I had learnt previously. On a technical level, what interested me most was the fact that GEE’s cloud-based computing power and its archive of satellite imagery, other socio-economic and geographic data provide particularly convenient conditions for realising spatio-temporal data studies. The fact that I don’t have to download large amounts of data to a local server or my own computer to call up data, process and analyse it, and also export the results, as well as use the archive function, greatly improves the efficiency of my research. I believe this is an important reason why it has become an effective tool for the remote sensing community.\nBut also because of its ability to focus on processing geospatial data, GEE’s connections to other broader subject areas are still worth exploring, such as biology and economics. In using GEE, I do not think that its visualisation capability is an advantage, at least not for 3D display. Also I learned that GEE’s modelling capabilities are actually limited (Zhao et al. 2021), for example in simulating complex geographic or ecological processes, such as modelling vegetation dynamics. This still requires more computational resources, open source datasets and the development of image processing algorithms to be integrated with assessment and management decisions to really help solve environmental problems."
  },
  {
    "objectID": "week5.html#references",
    "href": "week5.html#references",
    "title": "5  Week6-GEE",
    "section": "5.4 References",
    "text": "5.4 References\nAmani, M. et al., 2020. Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, årg. 13, s. 5326–5350.\nBullock, E.L., Woodcock, C.E. & Olofsson, P., 2020. Monitoring tropical forest degradation using spectral unmixing and Landsat time series analysis, Remote Sensing of Environment, årg. 238, s. 110968.\nBusker, T. et al., 2019. A global lake and reservoir volume analysis using a surface water dataset and satellite altimetry, Hydrology and Earth System Sciences. Copernicus GmbH, årg. 23, nr. 2, s. 669–690.\nChen, B. et al., 2017. A mangrove forest map of China in 2015: Analysis of time series Landsat 7/8 and Sentinel-1A imagery in Google Earth Engine cloud computing platform, ISPRS Journal of Photogrammetry and Remote Sensing, årg. 131, s. 104–120.\nHao, B. et al., 2019. Land Use Change and Climate Variation in the Three Gorges Reservoir Catchment from 2000 to 2015 Based on the Google Earth Engine, Sensors. Multidisciplinary Digital Publishing Institute, årg. 19, nr. 9, s. 2118.\nMeilianda, E. et al., 2019. Assessment of post-tsunami disaster land use/land cover change and potential impact of future sea-level rise to low-lying coastal areas: A case study of Banda Aceh coast of Indonesia, International Journal of Disaster Risk Reduction, årg. 41, s. 101292.\nWorkie, T.G. & Debella, H.J., 2018. Climate change and its effects on vegetation phenology across ecoregions of Ethiopia, Global Ecology and Conservation, årg. 13, s. e00366.\nXiong, J. et al., 2017. Automated cropland mapping of continental Africa using Google Earth Engine cloud computing, ISPRS Journal of Photogrammetry and Remote Sensing, årg. 126, s. 225–244.\nYu, L. & Gong, P., 2012. Google Earth as a virtual globe tool for Earth science applications at the global scale: progress and perspectives, International Journal of Remote Sensing. Taylor & Francis, årg. 33, nr. 12, s. 3966–3986.\nZhao, Q. et al., 2021. Progress and Trends in the Application of Google Earth and Google Earth Engine, Remote Sensing. Multidisciplinary Digital Publishing Institute, årg. 13, nr. 18, s. 3778."
  },
  {
    "objectID": "week7.html#summary",
    "href": "week7.html#summary",
    "title": "6  week7-Classification",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nRemote sensing classification is based on the principle of dividing data into meaningful parts, defining categories based on specific objectives, and slicing the data in different ways.\nI have summarised case studies of remote sensing classification applications in order to visualise the purpose and significance of the work.\n\nTable 1. Classification Case Summaries\n\n\n\n\n\n\n\n\n\nCase\nResearch interests\nSensors\nClassified object\nResearch target\n\n\n\n\n1\nUrban expansion\nLandsat\nUrban and non-urban areas\nAnalysing urban growth over time\n\n\n2\nAir pollution and land cover\nSentinel-3 (sea and land surface temperatures) and Sentinel-5 (major air pollutants)\nPollutants and land use\nUnderstanding the relationship between pollutants and land use change\n\n\n3\nUrban green space\nMedium-resolution imagery, hyperspectral data or various sensors such as LiDAR\nUrban green space\nFor biodiversity, recreation and urban planning\n\n\n4\nIllegal logging and forest monitoring\nCombination of sensors such as Landsat, MODIS\nTargeted areas affected by illegal logging\nInvestigation of environmental offences\n\n\n5\nForest fire\nLandsat TM\nFire hazard area\nCreating Fire Hazard Maps\n\n\n\n\n6.1.1 Classification method\n\nClassification and regression trees (CART)\n\n\nClassification trees\n\nPrinciples of building a classification tree:\n\n\n\n\n\n\n\nNo\nPrinciple\n\n\n\n\n1.start node\nThe top of the tree is the root node and contains all the data\n\n\n2.Segmentation\nThe Gini impurity criterion is used to select the best segmentation.\n\n\n3.Leaf nodes\nWhen further splits no longer increase the purity of a node, the node becomes a leaf node and corresponds to a decision result\n\n\n4.Pruning\nTo avoid overfitting, branches that do not significantly contribute to the model prediction can be removed\n\n\n\n\n\n\nFigure 1. Band-based CART Decision Tree for Distinguishing China Brazil Earth Resources Satellite (Wen, Hu & Yang 2008)\n\n\n\nTable 2. Classification tree features\n\n\n\n\n\n\nFeatures\nDefinitions\n\n\n\n\n1.Binary or multiple branches\nEach node can have two or more branches\n\n\n2.Decision rules\nAt each node, the tree uses features of the data to form rules\n\n\n3.Recursive splitting\nThe tree starts at the root node and recursively splits the dataset according to the best split point of the feature values.\n\n\n4.Node purity\nThe quality of the segmentation is evaluated using Gini impurity to maximise the purity of each node (when leaves with mixed results are encountered, the segmentation with the greatest improvement in purity can be chosen)\n\n\n\n\nRegression trees\n\nUnlike classification trees, which predict discrete values, regression trees predict numerical outcomes, such as the amount of pollution or test scores.\nPrinciples of building a regression tree\n\n\n\n\n\n\n\nNo.\nPrinciple\n\n\n\n\n1.Predicting continuous variables\nPredicting a continuous value through the tree structure\n\n\n2.Decision rules\nRules are formed at each decision node based on input features and then numerical outputs are predicted based on these rules.\n\n\n3.Segmentation Criteria\nThe total squared error (SSR) is used to decide where to segment the data in order to obtain the smallest possible error at each node.\n\n\n4.Tree Growth\nThe dataset is grown by recursively dividing it into more homogeneous subsets.\nEach split aims to maximise the uniformity of the nodes and minimise the error.\n\n\n\n\n\n\nFigure 2. A Sentinel-2 satellite image collection was used to create a sample collection containing labels of land cover types, and a CART classifier was trained to carry out a land cover classification study for Shenzhen\n\n\n\nLimitation\n\nCART is easily overfitted without proper constraints and pruning and the main strategies to avoid overfitting are:\n\n\n\n\n\n\n\nStrategy\nDefinition\n\n\n\n\nLimit growth\nSet a minimum number of observations (e.g. 20) on the leaves of the tree to prevent the tree from becoming too complex\n\n\nPruning\nAims to remove nodes to reduce model complexity, the above nodes have the characteristic of increasing model complexity without significantly improving predictive ability. Cost Complexity Pruning is used to balance the fit and complexity of the tree by considering the complexity parameter alpha\n\n\nCross-validation\nRepeat the modelling process on different training and test datasets, use cross-validation (e.g.10-fold cross-validation) to assess the generalisation ability of the model, and select the model with the lowest error on the test data\n\n\n\n\nRandom forest (RF)\n\nRF are an integrated learning method, mainly for classification (and regression), which runs by building a large number of decision trees during training and outputting the classes\n\nTable 3. Step-by-step table\n\n\n\n\n\n\n\nNo\nStep\nPrinciple\n\n\n\n\n1\nBootstrap sampling\nFor each tree, a random sample of data (70%) is selected for replacement and 30% of the data is left out, called out-of-bag (OOB) data\n\n\n2\nRandom feature selection to build the tree\nA random subset of features is selected at each split to introduce additional randomness, ensuring that the trees are de-correlated and adding to the overall diversity\n\n\n3\nAggregation\nPrediction by aggregation integration\n\n\n4\nOut-of-bag error estimation\nData that was omitted during tree construction can be used to obtain an unbiased estimate of classification accuracy as out-of-bag error\n\n\n5\nAssessment of feature importance\nDetermined by the extent to which each feature reduces weighted impurities in the tree, averaged over all trees\n\n\n\n\n\n\nFigure 3.RF classifier is an ensemble method that trains several decision trees in parallel with bootstrapping followed by aggregation.(Misra & Li 2020)\n\n\n\nAdvantages\n\nRandom forests combine the results of individual trees into a more general model to deal with overfitting. It has more advantages：\n\n\n\n\n\n\n\nAdvantages\nDefinitions\n\n\n\n\nRobustness\nCombining the predictions of many trees is more robust and accurate than a single decision tree.\n\n\nVersatility\nTree models are able to deal with complex non-linear relationships by branching multiple times, which provides a basis for dealing with classification problems (outputting the plurality of categories) and regression problems (outputting mean predictions).\n\n\n\n\n\n\nFigure 4. A Sentinel-2 satellite image collection was used to create a sample collection containing labels of land cover types, and a CART classifier was trained to carry out a land cover classification study for Shenzhen\n\n\n\nMaximum likelihood method (MLM) and Support Vector Machine (SVM)\n\n\nComparison of the MLM and SVM\n\nApplicability: MLM is suitable for cases where statistical properties are obvious and a priori information is available, while SVM is more powerful when dealing with datasets with complex boundaries or non-linear distributions (higher dimensional).\nRobustness: Compared to MLM, which is more sensitive to outliers in the training data, SVM is usually more robust to outliers when using soft spacing.\nThe characteristics of each are as follows：\n\nTable 4. Maximum likelihood method (probability-based decision rule classifier)\n\n\n\n\n\n\nFeature\nDefinition\n\n\n\n\nDecision Making\nBy comparing the feature statistics (e.g. mean and standard deviation) of a given pixel with the probability density function of each land cover type, the category with the highest probability is selected for the classification of that pixel\n\n\nThreshold Setting\nSupports setting a probability threshold below which pixels are not classified to prevent classification uncertainty\n\n\nPrior probability\nIntegrates a prior probability information, where the pre-known land cover type can be used in the classification process\n\n\n\n\nTable 5. Support Vector Machine (Linear Binary Classifier)\n\n\n\n\n\n\nFeature\nDefinition\n\n\n\n\nDecision Making\nFinds a maximum interval (determined by the nearest support vector) to distinguish between two classes of training data\n\n\nSoft Interval\nAllow a certain amount of classification error, determine how much misclassification is allowed by cross validation\n\n\nKernel Trick\nIf the data is not linearly separable, map the data to a higher dimensional space via a kernel function to achieve linear separability\n\n\nHyperparameters\nAdjust hyperparameters such as the penalty coefficient and the kernel function parameter to optimise"
  },
  {
    "objectID": "week7.html#applications",
    "href": "week7.html#applications",
    "title": "6  week7-Classification",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nThe methods of remote sensing classification have been in a developmental stage. The initial classification of remote sensing images was actually achieved through visual interpretation, mainly based on human experience and knowledge to interpret basic elements and specific signs to achieve the purpose of identifying the type of feature (Jia et al. 2011).\nIn order to meet the requirement of providing classification accuracy, remote sensing researchers have been working on classification techniques and methods. For example initially developed various unsupervised and supervised classification techniques learnt this week, which can also be classified as parametric vs. non-parametric classifiers (Xu et al. 2005). In fact advanced classification algorithms such as neural networks, support vector machines, and expert systems have been widely used for a long time in the practice of classification such as urban planning and natural disaster monitoring.\nWhat intrigued me was the integration of multiple learned classification techniques. Du et al. (Du et al. 2012) combined the advantages of support vector machines in solving the difficulties of small sample sizes and poor generalisation with the advantages of random forests being able to deal with hyperspectral imagery, and used a dynamic selection of a more appropriate specific classifier for each unclassified pixel to conduct AVIRIS image from Indian Pines Classification accuracy experiments were conducted on AVIRIS image from Indian Pines. It was found that the classification accuracy of the combination of the two classifiers was more accurate than the classification results with Support Vector Machines and Random Forests respectively. Such a study provides a solution to improve the classification accuracy of hyperspectral classification, as well as a reference for us to understand the characteristics and differences between the two specific classifiers.\n\n\n\nFigure 5. Classification Results of AVIRIS image using a combination of Support Vector Machines and Random Forests (Du et al. 2012)\n\n\nHowever, remote sensing image classification methods are still affected by many factors, such as complex surface information, and classification applications still face challenges. For example, Cheng et al. (Cheng et al. 2020) summarised the challenges in scene classification such as high diversity within classes, high similarity between classes, large differences in object proportions, and coexistence of multiple ground objects. Despite the rapid development of deep learning technology applications, as well as the existence of: 1. large datasets without clean labels, fully supervised learning is difficult to work; 2. poor model generalisation and many other problems. This illustrates the huge gap that still exists between the current machine and human understanding level performance. Therefore continued optimisation of deep learning driven remote sensing image classification in the future remains a worthwhile development."
  },
  {
    "objectID": "week7.html#reflection",
    "href": "week7.html#reflection",
    "title": "6  week7-Classification",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nThis week I was exposed to an important part of processing remote sensing data, which is remote sensing image classification. I think this technique is practical and varied.\nRemote sensing image classification is actually the classification of each pixel point or each area in an image into different categories according to some rules based on their spectral features, spatial structure features or other information in different spectral bands. This is particularly important in the application of remote sensing data, because it is a conversion step to use remote sensing images as a tool to solve specific geographic problems, and I believe that every application of remote sensing will use this step of classification.\nClassification is diverse and there are different classification methods and approaches to choose for different classification tasks, from maximum likelihood classification to random forests and support vector machines, I find that research is needed to explore how to choose the right classification algorithms to ensure the accuracy of remote sensing data analysis. The choice of algorithm depends not only on the data characteristics (e.g., is it hyperspectral remote sensing?) , but also depends on the needs of the specific problem.\nDue to the wide range of spatial and temporal characteristics of remote sensing data, I believe that the trend of combining remote sensing classification with deep learning will continue in the future. I am very willing to learn and optimise classification methods, and although it is now possible to download finished remote sensing data that have been classified by different teams, e.g. land cover data, I believe that it is very valuable to have access to first-hand data and to process it."
  },
  {
    "objectID": "week7.html#references",
    "href": "week7.html#references",
    "title": "6  week7-Classification",
    "section": "6.4 References",
    "text": "6.4 References\nCheng, G. et al., 2020. Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, årg. 13, s. 3735–3756.\nDu, P. et al., 2012. Hyperspectral remote sensing image classification based on the integration of support vector machine and random forest [Online].\nJia, K. et al., 2011. A Review of Classification Methods of Remote Sensing Imagery, SPECTROSCOPY AND SPECTRAL ANALYSIS. Beijing: Office Spectroscopy & Spectral Analysis, årg. 31, nr. 10, s. 2618–2623.\nMisra, S. & Li, H., 2020. Chapter 9 - Noninvasive fracture characterization based on the classification of sonic wave travel times [Online]. I: S. Misra, H. Li & J. He red. Machine Learning for Subsurface Characterization. Gulf Professional Publishing, s. 243–287.\nWen, X., Hu, G. & Yang, X., 2008. CBERS-02 Remote Sensing Data Mining Using Decision Tree Algorithm [Online].\nXu, M. et al., 2005. Decision tree regression for soft classification of remote sensing data, Remote Sensing of Environment, årg. 97, nr. 3, s. 322–336."
  },
  {
    "objectID": "week8.html#summary",
    "href": "week8.html#summary",
    "title": "7  week8-Classification II",
    "section": "7.1 Summary",
    "text": "7.1 Summary\n\n7.1.1 Advanced Classification Methods\nWe have continued to learn about land cover classification methods and also learnt to cope with different data, compared to what we learnt last week, this week is mainly about how to deal with different objects.\n\nObject-Based Image Analysis (OBIA)\n\nOBIA is based on the geometrical and spectral properties of objects in an image for analysis and classification.\nThe main principle of the method is related to hyperpixels. It aggregates neighbouring image elements with similar properties into hyperpixels, which are used to represent the actual objects on the ground. The hyperpixels are in turn generated by the Simple Linear Iterative Clustering algorithm. This algorithm calculates spatial distances and colour differences among regular points on the image, which can determine which image elements should be combined into a single hyperpixel, typically after 4-10 iterations, to better reflect the shape and size of the target object.\n\n\n\nAfter pre-processing and feature extraction with spectral gradient and K-means clustering, simple non-iterative clustering and calculation of NDVI, super-pixel segmentation, training of CART using training data, classification of multi-band images (I think further optimisation is needed for image segmentation of classification objects)\n\n\n\nAdvantages:\n\n\n\n\n\n\n\n\n1.\nCan more accurately represent the actual objects on the ground such as buildings, more accurate classification results\n\n\n2.\nHas a variety of classifiers and software packages that can use different algorithms to provide more complex analyses and functions, such as Supercells package can use any distance measure\n\n\n\n\nLimitations:\n\n\n\n\n\n\n\n\n1.\nDoes not take into account the connectivity between superpixels, which may lead to very small and difficult to interpret classification regions\n\n\n2.\nusing the Euclidean distance metric limits the suitable data types and classification needs.\n\n\n3.\nPerformed in LAB colour space, which requires conversion of the raw data, affecting the accuracy of the classification.\n\n\n\n\nSubpixel analysis\n\nThis method is mainly used when a single pixel contains multiple land cover types and is also called spectral mixture analysis (SMA). Rather than assigning an entire pixel to one dominant land cover class, it estimates the proportion of each land cover type in the pixel.\nThe basic principle is that the reflectance captured by a pixel is a combination of the reflectance of all land cover types (endmembers) within that pixel. By using the purely spectral characteristics of these endmembers, the analysis attempts to reverse derive the proportion of each endmembers contribution to the observed pixel value.\n\n\n\nUsing Dar-es-salaam as the study area, the sub-pixel analysis was completed by using Landsat 8 with dynamic end-element extraction, calculating the average spectral value of the delineated area, and calculating the percentage of each pixel in the image for each end-element type.\n\n\n\n\n\nThe subpixel images were enhanced and classified for each pixel with the largest percentage of land cover (e.g., 50% or more). Reducing the subpixel results to a single classification prepares the way for later use of traditional error matrix methods to assess classification accuracy.\n\n\n\nAdvantages:\n\n\n\n\n\n\n\n\n1.\nProvides a more detailed understanding of land cover by disaggregating the components within a pixel, further effectively improving the resolution of land cover information compared to full pixel classification\n\n\n2.\nbetter capture minor changes in the landscape as the land is monitored over time\n\n\n\n\nLimitations:\n\n\n\n\n\n\n\n\n1.\nComputationally complex, the more end-elements considered the more computationally intensive\n\n\n2.\nAccuracy depends on the purity of the end elements. Impure spectral features can lead to unreliable results\n\n\n3.\nAnalyses are limited by spectral library data, so comprehensive spectral libraries are needed as much as possible\n\n\n\n\n\n7.1.2 Assessment of classification accuracy\n\nConfusion Matrix\n\nConfusion matrices are often used when performing accuracy assessments, being able to compare the classification results with an independent dataset of real ground conditions, providing a detailed view of the classification performance including: true positives (TP), false positives (FP), true negatives (TN) and false negatives (FN). And the accuracy values can be obtained through the confusion matrix.\n\nClassification of accuracy values\n\n\nTable of accuracy value type\n\n\n\n\n\n\nAccuracy value\nDefinition\n\n\n\n\nProducer’s accuracy\nA measure of accuracy from the producer’s perspective. Indicates the ratio of pixels that are correctly classified to all relevant pixels in that category (i.e., the sum of true positives and false negatives)\n\n\nUser accuracy\nMeasures the ratio of correctly classified pixels to all pixels classified as that category (i.e., the sum of true positives and false positives). User accuracy is concerned with whether a pixel classified as a category actually belongs to that category\n\n\nOverall Accuracy\nIndicates the proportion of correctly classified pixels across all categories and reflects the overall performance of the classification model\n\n\n\nIn fact, application scenarios as well as requirements have to be considered when assessing the accuracy of classification models. For example, for land cover change detection, high overall accuracy may be most important. And for environmental monitoring, user accuracy may be more critical.\nGenerally the Kappa coefficient can be used to assess the difference between classification accuracy and random classification accuracy, with higher values indicating better classification accuracy and greater likelihood of exceeding random classification. However, it has also been argued (Foody 2020) that the Kappa coefficient does not directly reflect accuracy; it is simply a measure of classification consistency beyond the random level, and can indicate either consistency obtained by chance alone or almost perfect consistency, and the interpretation of its true significance is often ambiguous.\n\nValidation data with cross-validation methods\n\n\nLeave-One-Out Cross-Validation\n\nThe number of folds is equal to the number of data points, and each iteration uses all but one data point for training and the missed points for testing. While maximising the amount of data used for training, the number of iterations is high and not practical for large datasets.\n\nSpatial cross-validation\n\nDue to the first law of geography (observations close to each other are more likely to be similar), the method spatially partitions the data so that the training and validation sets are not geographically intersected to avoid due to spatial autocorrelation.\n\nCharacteristics of Spatial cross-validation\n\n\n\n\n\n\nCharacteristic\nDefinition\n\n\n\n\n1. Advantage\nin reducing the risk of overfitting the model to spatially similar data\n\n\n2. Requirement\nSufficient data distribution is needed to create meaningful spatial partitions\n\n\n3. Optimisation\nthe hyperparameters of the SVM classifier need to be tuned to balance the model complexity and generalisation capabilities\n\n\n4. Multiple iterations\nMultiple rounds of spatial partitioning and validation are required to ensure robustness"
  },
  {
    "objectID": "week8.html#applications",
    "href": "week8.html#applications",
    "title": "7  week8-Classification II",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nIn conjunction with this week’s classification method, I have taken a look at its position in the development of remote sensing applications.\nLand use information has been extracted in different ways at different times in the development of remote sensing technology. In the early days, images captured by remote sensing sensors had larger pixels than ground features at low and medium resolution, so subpixel analysis was needed to respond to land class features(Blaschke 2010). However, in the gradual increase of high-resolution imagery applications, the pixel-based analysis method is not able to respond to the spatial variations of land cover, so object-based image analysis becomes an effective method in high spatial resolution imagery applications. Due to its spectral, spatial, textural, and topological characteristics, and its ability to respond to geographic information, there has been a gradual increase in the number of studies and techniques related to geographic object-based image analysis (GEOBIA) (Rees 1989).\nManne et al. (Suneetha et al. 2020) used object-based image analysis with three separate stages of segmentation, generation of training data and final classification to obtain the range of forestry based on remote sensing satellite images such as Deimos-2 and Cartosat-1. As learnt in the course. The measurement of the classification results was determined from four main aspects, namely, Producers Accuracy, Users Accuracy, Overall Accuracy and Kappa coefficient value. Although the accuracy value of the data in Cartosat-1 is low and the interpretation of the Kappa coefficient is debatable. However, it can show the usefulness of the method for multispectral image classification and can support the real needs such as ecological monitoring.\n\n\n\nClassified map of Deimos-2 using OBIA technique with radius 5 in (a) and 10 in (b) (Suneetha et al. 2020)\n\n\n\n\n\nAccuracy assessment of OBIA: Deimos-2 (R 10) 150 samples (Suneetha et al. 2020)\n\n\nHossain et al. (Hossain & Chen 2019) summarised the research trends in image analysis based on geographic objects, and were able to find that the ability to match objects segmented from images with meaningful geographic objects is the key challenge in this field, and we can intuitively see that the cases and tools for this classification technique are improving significantly, so I also believe that continuous improvement of image segmentation techniques is also the future direction of research.\n\n\n\nThe amount of GEOBIA literature and some associated triggers(Hossain & Chen 2019)"
  },
  {
    "objectID": "week8.html#reflection",
    "href": "week8.html#reflection",
    "title": "7  week8-Classification II",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nAfter two weeks of learning about the classification process in remote sensing in detail, last week focused on the basic methods of classification such as CART, Random Forest and Support Vector Machines. This was mainly about classifying for single pixels, and this week further emphasised on classifying for super-pixel cases, and sub-pixel cases, as a step-by-step process. It had not occurred to me last week that there would also be classification for different objects, and this week’s classification study has inspired me. Indeed, in practical remote sensing applications, the needs are constantly increasing, so the classification methods are constantly improving. I think in the future, continue to improve the robustness of multispectral image segmentation algorithms, improve the edge, time and other feature sets, and improve the classification rules of decision trees(Liu et al. 2006) will have continued research value in the field, and I also hope that more learning based on the object to complete the ecological monitoring aspects of the work, which is greatly beneficial to sustainable development and disaster monitoring.\nMeanwhile after a few weeks of systematic learning, we had a general understanding of the process of processing remote sensing data once we got it, just like the process of classification, which is from class definition, preprocessing, training, pixel assignment, and then accuracy assessment, and in the practical exercises, we also used GEE to understand how most of them work, which helped us to understand what we had learnt in the class. I also learnt through searching for applications that despite the different objects being classified, the classification tool methods used (e.g. decision trees) are common, which more than anything else allowed me to tie the two weeks of classes together.\nThis week’s course also brought us a lot of dialectical thinking, through the study of accuracy verification I learnt that the starting point for thinking about a problem and evaluating the results is different for different objects (producers and users), and that we have different criteria for evaluating the results in different application scenarios. As in the example of kappa coefficients, their interpretative and imperative nature is not entirely certain, despite the fact that many scholars see them as agreed-upon steps. I think this is the idea that the instructor has been instilling in this course, to maintain CRITICAL THINKING, even in published articles."
  },
  {
    "objectID": "week8.html#references",
    "href": "week8.html#references",
    "title": "7  week8-Classification II",
    "section": "7.4 References",
    "text": "7.4 References\nBlaschke, T., 2010. Object based image analysis for remote sensing, ISPRS Journal of Photogrammetry and Remote Sensing, årg. 65, nr. 1, s. 2–16.\n\nFoody, G.M., 2020. Explaining the unsuitability of the kappa coefficient in the assessment and comparison of the accuracy of thematic maps obtained by image classification, Remote Sensing of Environment, årg. 239, s. 111630.\n\nHossain, M.D. & Chen, D., 2019. Segmentation for Object-Based Image Analysis (OBIA): A review of algorithms and challenges from remote sensing perspective, ISPRS Journal of Photogrammetry and Remote Sensing, årg. 150, s. 115–134.\n\nLiu, Y. et al., 2006. Review of remotely sensed imagery classification patterns based on object-oriented image analysis, Chinese Geographical Science, årg. 16, nr. 3, s. 282–288.\n\nRees, G., 1989. Remote Sensing - Digital image processing in remote sensing., Polar Record, årg. 25, nr. 152, s. 67–67.\n\nSuneetha, M. et al., 2020. Object based Classification of Multispectral Remote Sensing Images for Forestry Applications [Online]. New York, NY, USA: Association for Computing Machinery."
  },
  {
    "objectID": "week9.html#summary",
    "href": "week9.html#summary",
    "title": "8  week9-SAR",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nThis week is about learning about a specific remote sensing product and its application, Synthetic Aperture Radar (SAR), which is an active remote sensing technique that uses radar to create two-dimensional images or three-dimensional reconstructions of objects. It has several characteristics:\n\n8.1.1 Features and Benefits\n\nSAR features and benefits\n\n\n\n\n\n\nFeature and benefit\nDefinition\n\n\n\n\nActive Sensor\nUnlike passive sensors that rely on sunlight, the SAR comes with its own light source. Images can be acquired at any time or season\n\n\nSurface texture data\nThanks to its high-resolution capabilities, SAR can detect fine details about the surface texture\n\n\nCloud penetration\nSAR can penetrate weather and clouds because radar wavelengths can penetrate atmospheric conditions that block optical sensors\n\n\nWavelength Variation\nDifferent wavelengths (L, C, S, X, Ku, K and Ka bands) are used for various applications. The wavelength determines the depth of penetration and resolution of the image\n\n\n\n\n\n8.1.2 Working Principle\nDue to the special way in which SAR transmits and acquires radar signals, it has specialised terminology, which belongs to and reflects the principles of its operation:\n\nSAR terms and interpretation\n\n\nTerm\nInterpretation\n\n\n\n\nBackscatter(Amplitude)\nThis refers to the strength of the signal reflected back to the radar antenna from the surface, and can provide information about the characteristics of the ground being illuminated, such as surface roughness, humidity and vegetation type. It is directly related to the surface characteristics and also to the polarisation state of the radar wave.\n\n\nPolarization\nThe direction of electromagnetic wave vibration when radar waves are transmitted and received. Radar systems can transmit and receive waves with different polarisations, which is called polarisation diversity (Ruan et al. 2011). Polarisation can directly affect backscatter, the amplitude of backscatter is affected by the scattering mechanism, and different surface types (water, bare soil, vegetation) respond differently to radar waves of different polarisation states. Also the information richness of backward scattering can vary depending on the polarisation used.\n\n\nDielectric constant\nA measure of the ability of a substance to transmit electricity without conduction. It affects how radar signals are scattered and so is also directly related to Backscatter.\n\n\nPhase\nRefers to the position of a wave in its cycle as it returns to the sensor and is critical to interferometry (Zhang et al. 1998).\n\n\n\n\n\n8.1.3 Processing flow\nAfter learning the basic principles and terminology of SAR, we step by step understand the basic process of processing SAR data:\n\n\n\n\n\n\n\nStep\nInterpretation\n\n\n\n\nDetermine the I and Q values\nContains the real part (I) and imaginary part (Q), which represents the In-phase (In-phase) and Quadrature (Quadrature) pairs, used to characterise the complex form of the received electromagnetic wave.\n\n\nGeneration of visual image (amplitude)\nGround Range Detected (GRD) is used. In Google Earth Engine (GEE), this is achieved by squaring and root-checking the I and Q values of the Single Look Complex (SLC) to calculate the reflected intensity (backscatter).\n\n\nStorage of values\nBackscatter values can be stored as power, amplitude or decibels (dB). These are suitable for statistical analysis, visualisation and identification of differences in dark areas (e.g. water bodies) respectively.\n\n\nAveraging of multiple images\nNoise in SAR images is caused by uneven scattering and can be smoothed by averaging multiple images.\n\n\n\nIn practice, processing SAR data for change detection takes into account both the statistical and noise properties of the images. It is often necessary to collect data at multiple time points and use statistical methods to assess the significance of changes (e.g. the standard distribution may only apply to the difference image and not the original image (Canty 2019)). It is also necessary to use classification thresholds to optimise the accuracy of change detection and to validate the results with ROC curves. In some cases, image fusion techniques such as combining the intensity of the SAR data with the hue and saturation of the optical image (classified as decision-level fusion, object-level fusion, and image fusion depending on the layer used and the object generated) will also be utilised to improve the visualisation of change detection."
  },
  {
    "objectID": "week9.html#applications",
    "href": "week9.html#applications",
    "title": "8  week9-SAR",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nThis course demonstrates the benefits of SAR data, reflecting a wide range of potential application scenarios. However, it is a fact that SAR images are poor in target identification and band information (Li et al. 2022). Therefore, researchers usually fuse SAR images with other remote sensing images for better visual performance and additional valuable information, which are mainly divided into: 1. fusion between SAR images; 2. fusion with optical images; 3. and also fusion with other data.\nFusion between SAR images can obtain more band and polarisation information (Zhang et al. 2022). Li et al. (Li et al. 2022) used the non-subsampled shear transform (NSST) to fuse the decomposed subband images through the coefficient of variation and phase consistency weighted fusion rules, and proceeded to the fused images based on the extracted band and polarisation information, which was found to retain more valuable information as well as being easy to interpret. This study also highlights the NSST method with PCA fusion and wavelet fusion and other fusion methods mentioned in the classroom image fusion methods that can visualise the differences between the different methods.\n\n\n\nComparison of representative regions and their integration results (Li et al. 2022)\n\n\nSince the imaging modes of SAR and optical images are different, Chu et al. (Chu et al. 2020) also used NSST and an improved SAR optical image fusion algorithm to avoid the usual problems of spectral distortion and excessive noise introduction in the fused image, and smoothly integrate the SAR image into the optical image. In fact, the application of SAR and optical images is more practical and can be widely used in diverse fields such as urban built-up area research (Teimouri, Mokhtarzade & Valadan Zoej 2016), agricultural cultivation research (Liu et al. 2019), glacier ecological monitoring (Shah, Jayaprasad & James 2019), and so on.\n\n\n\nNovel fusion method for SAR and optical images based on non-subsampled shearlet transform (Chu et al. 2020)\n\n\nIn class the instructor also showed a case study combining SAR data with statistical data (Open Access Damage Detection Using Sentinel-1 Imagery). The project utilised a combination of statistical tests such as SAR and ground truth data, using a pixel t-test to determine if the pixels had changed significantly since the Beirut bombing. The effectiveness of the algorithm was demonstrated by combining SAR image changes with UN losses in the placebo test. This fully demonstrates the advantages of SAR to be able to perform the mission of detailed change detection in disaster scenarios.\n\n\n\nPixelwise T-Test to determine pixel changes from the 2020 Beirut bombing (Open Access Damage Detection Using Sentinel-1 Imagery)\n\n\nBased on the above applications, I believe that the joint use of optical and SAR remote sensing data in the future, the application of multi-band multi-dimensional SAR has been the trend of the application of this product, and at the same time, because of its working mechanism that is not affected by the weather time conditions, it is very important for the finer and long time monitoring tasks."
  },
  {
    "objectID": "week9.html#reflection",
    "href": "week9.html#reflection",
    "title": "8  week9-SAR",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nUp to this point, we have learnt about everything from understanding remote sensing data, to pre-processing, to using GEE for some simple classification, and then this week furthering our understanding of a specific remote sensing product, SAR.\nIn this chapter, the benefits of SAR, such as the high resolution capabilities and the ability to operate year round, were specifically demonstrated, and the instructor also brought the benefits of the product to life through a specific case study of the Beirut bombing. Among them, I was amazed by SAR’s outstanding performance in terms of high accuracy. For example, in the study (Ballinger & Donchyts 2023) mentioned by the professor, the damage to the buildings in the city damaged during the war in Ukraine was monitored using the pixel dimension T-test algorithm, and at the same time it was able to generate a real-time map of the damage, and this real-time monitoring function and level of refinement was breathtaking to me.\nAs the teacher threw out the question, “SAR is very useful because it can penetrate the clouds, unlike optical sensors (e.g. Landsat), but how is it useful for our analysis?” In the example above it is illustrated that the use of SAR means that monitoring models can be deployed almost anywhere on the planet regardless of atmospheric conditions and time of day, as well as ingesting new imagery in a timely and increasingly accurate manner, which I think is extremely helpful for disaster prevention and mitigation, post-disaster post-war monitoring, and ecological monitoring applications.\nI am also concerned that SAR is also capable of monitoring the growth of agricultural crops over the entire climatic period, and by combining it with optical data, it can realise effective monitoring of agricultural targets. I think this technology is particularly important for the agricultural development of my home country China, which, as a large agricultural country, has a great demand for precision agriculture, and I can hope to apply remote sensing technology to enhance the development of the agricultural economy. I also hope to apply remote sensing technology to enhance the development of agricultural economy.\nAfter a semester of Remotely Sensing Cities and Environments, I have slowly gained a new understanding of this field from a beginner in remote sensing, and have learnt about the concrete usefulness and infinite possibilities of remote sensing for the real society. I hope that in the future remote sensing technology will be known to more people and become a technology integrated into everyone’s life. I would like to thank the teacher for his careful preparation of this course and hope that the Earth will become better!"
  },
  {
    "objectID": "week9.html#references",
    "href": "week9.html#references",
    "title": "8  week9-SAR",
    "section": "8.4 References",
    "text": "8.4 References\nBallinger, D.O. & Donchyts, D.G., 2023. A New Algorithm for Persistent Building Damage Detection in Synthetic Aperture Radar Imagery.\nCanty, M.J., 2019. Image Analysis, Classification and Change Detection in Remote Sensing: With Algorithms for Python, Fourth Edition. 4. udgave. Boca Raton: CRC Press.\n\nChu, T. et al., 2020. Novel fusion method for SAR and optical images based on non-subsampled shearlet transform, International Journal of Remote Sensing. Taylor & Francis, årg. 41, nr. 12, s. 4590–4604.\n\nLi, X. et al., 2022. Multi-Band and Polarization SAR Images Colorization Fusion, Remote Sensing. Multidisciplinary Digital Publishing Institute, årg. 14, nr. 16, s. 4022.\n\nLiu, C. et al., 2019. Research advances of SAR remote sensing for agriculture applications: A review, Journal of Integrative Agriculture, årg. 18, nr. 3, s. 506–525.\n\nRuan, X. et al., 2011. Performance Experiment of Classification Using Chinese Airborne Multi-Band and Multi-Polar SAR Data [Online]. Tengchong, Yunnan, China: IEEE.\n\nShah, E., Jayaprasad, P. & James, M.E., 2019. Image Fusion of SAR and Optical Images for Identifying Antarctic Ice Features, Journal of the Indian Society of Remote Sensing, årg. 47, nr. 12, s. 2113–2127.\n\nTeimouri, M., Mokhtarzade, M. & Valadan Zoej, M.J., 2016. Optimal fusion of optical and SAR high-resolution images for semiautomatic building detection, GIScience & Remote Sensing. Taylor & Francis, årg. 53, nr. 1, s. 45–62.\n\nZhang, W. et al., 2022. Adaptive Contourlet Fusion Clustering for SAR Image Change Detection, IEEE Transactions on Image Processing, årg. 31, s. 2295–2308.\n\nZhang, Y. et al., 1998. SAR interferometry: phase unwrapping by fringe-line detection [Online]. SPIE."
  }
]